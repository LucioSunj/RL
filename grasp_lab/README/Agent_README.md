æˆ‘æ¥è¯¦ç»†è§£é‡Šè¿™ä¸ªPPO Agentçš„å®ç°ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œæ•´ä¸”è§„èŒƒçš„PPOç®—æ³•å®ç°ã€‚

## ğŸ—ï¸ **æ•´ä½“æ¶æ„æ¦‚è§ˆ**

```python
PPO Agent æ¶æ„
â”œâ”€â”€ RolloutBuffer (ç»éªŒç¼“å†²åŒº)
â”‚   â”œâ”€â”€ å­˜å‚¨ç»éªŒæ•°æ®
â”‚   â”œâ”€â”€ è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥  
â”‚   â””â”€â”€ æ‰¹æ¬¡æ•°æ®é‡‡æ ·
â””â”€â”€ PPOAgent (æ ¸å¿ƒç®—æ³•)
    â”œâ”€â”€ ActorCritic ç½‘ç»œ
    â”œâ”€â”€ ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    â”œâ”€â”€ ç»éªŒæ”¶é›†å’Œå­˜å‚¨
    â”œâ”€â”€ ç½‘ç»œæ›´æ–°è®­ç»ƒ
    â””â”€â”€ æ¨¡å‹ä¿å­˜åŠ è½½
```

## ğŸ“¦ **1. RolloutBuffer - ç»éªŒç¼“å†²åŒº**

### **æ ¸å¿ƒåŠŸèƒ½**
```python
class RolloutBuffer:
    def __init__(self, buffer_size: int, observation_space, action_space, device='cpu'):
        # ç¼“å†²åŒºå¤§å°é…ç½®
        self.buffer_size = buffer_size  # é€šå¸¸æ˜¯2048
        self.ptr = 0                    # å½“å‰å†™å…¥ä½ç½®
        self.size = 0                   # å½“å‰æ•°æ®é‡
```

### **æ•°æ®å­˜å‚¨ç»“æ„**
```python
# ä¸ºæ¯ç§è§‚å¯Ÿç±»å‹åˆ†é…å­˜å‚¨ç©ºé—´
self.observations = {}
for key, space in observation_space.spaces.items():
    self.observations[key] = torch.zeros(
        (buffer_size, *space.shape), dtype=torch.float32, device=device
    )

# æ ¸å¿ƒRLæ•°æ®
self.actions = torch.zeros((buffer_size, action_space.shape[0]), ...)     # åŠ¨ä½œ
self.rewards = torch.zeros(buffer_size, ...)                              # å¥–åŠ±
self.values = torch.zeros(buffer_size, ...)                               # ä»·å€¼ä¼°è®¡
self.log_probs = torch.zeros(buffer_size, ...)                            # åŠ¨ä½œæ¦‚ç‡å¯¹æ•°
self.dones = torch.zeros(buffer_size, dtype=torch.bool, ...)              # ç»“æŸæ ‡å¿—
self.advantages = torch.zeros(buffer_size, ...)                           # ä¼˜åŠ¿å‡½æ•°
self.returns = torch.zeros(buffer_size, ...)                              # å›æŠ¥
```

### **GAEä¼˜åŠ¿å‡½æ•°è®¡ç®—** (æ ¸å¿ƒç®—æ³•)
```python
def compute_advantages_and_returns(self, last_value, gamma=0.99, gae_lambda=0.95):
    """
    GAE (Generalized Advantage Estimation) ç®—æ³•
    """
    advantages = torch.zeros_like(self.rewards)
    returns = torch.zeros_like(self.rewards)
    
    gae = 0
    next_value = last_value
    
    # ä»åå¾€å‰è®¡ç®— (Temporal Difference Learning)
    for step in reversed(range(self.size)):
        # åˆ¤æ–­æ˜¯å¦ä¸ºç»ˆç«¯çŠ¶æ€
        if step == self.size - 1:
            next_non_terminal = 1.0 - self.dones[step].float()
        else:
            next_non_terminal = 1.0 - self.dones[step + 1].float()
        
        # TD Error: Î´ = r + Î³V(s') - V(s)
        delta = self.rewards[step] + gamma * next_value * next_non_terminal - self.values[step]
        
        # GAEé€’æ¨å…¬å¼: A^{GAE} = Î´ + Î³Î»A^{GAE}_{t+1}
        gae = delta + gamma * gae_lambda * next_non_terminal * gae
        advantages[step] = gae
        
        # Return = Advantage + Value
        returns[step] = advantages[step] + self.values[step]
        next_value = self.values[step]
    
    # ä¼˜åŠ¿å‡½æ•°æ ‡å‡†åŒ– (é‡è¦çš„ç¨³å®šæ€§æŠ€å·§)
    self.advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    self.returns = returns
```

**GAEå…¬å¼è§£é‡Š**:
- **Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)**: TDè¯¯å·®
- **A^{GAE}_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)^2Î´_{t+2} + ...**: GAEä¼˜åŠ¿å‡½æ•°
- **Î»**: æ§åˆ¶bias-variance trade-offçš„å‚æ•°

## ğŸ§  **2. PPOAgent - æ ¸å¿ƒç®—æ³•ç±»**

### **åˆå§‹åŒ–é…ç½®**
```python
def __init__(self,
             observation_space,
             action_space,
             lr_actor: float = 3e-4,        # Actorå­¦ä¹ ç‡
             lr_critic: float = 3e-4,       # Criticå­¦ä¹ ç‡
             gamma: float = 0.99,           # æŠ˜æ‰£å› å­
             gae_lambda: float = 0.95,      # GAE Î»å‚æ•°
             clip_epsilon: float = 0.2,     # PPOè£å‰ªå‚æ•°
             entropy_coef: float = 0.01,    # ç†µæ­£åˆ™åŒ–ç³»æ•°
             value_loss_coef: float = 0.5,  # ä»·å€¼æŸå¤±æƒé‡
             max_grad_norm: float = 0.5,    # æ¢¯åº¦è£å‰ª
             target_kl: float = 0.01,       # KLæ•£åº¦é˜ˆå€¼
             n_epochs: int = 10,            # æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°
             batch_size: int = 64,          # æ‰¹é‡å¤§å°
             buffer_size: int = 2048,       # ç¼“å†²åŒºå¤§å°
             device: str = 'auto'):
```

### **ç½‘ç»œå’Œä¼˜åŒ–å™¨è®¾ç½®**
```python
# Actor-Criticç½‘ç»œ
self.actor_critic = ActorCritic(observation_space, action_space).to(self.device)

# åˆ†åˆ«ä¸ºActorå’ŒCriticè®¾ç½®ä¼˜åŒ–å™¨
self.optimizer = optim.Adam([
    {'params': self.actor_critic.actor.parameters(), 'lr': lr_actor},
    {'params': self.actor_critic.critic.parameters(), 'lr': lr_critic}
])

# å­¦ä¹ ç‡è¡°å‡è°ƒåº¦å™¨
self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=1000, gamma=0.9)
```

## ğŸ¯ **3. åŠ¨ä½œé‡‡æ ·æœºåˆ¶**

```python
def get_action(self, observation: Dict, deterministic: bool = False):
    """
    è·å–åŠ¨ä½œ - æ”¯æŒç¡®å®šæ€§å’Œéšæœºç­–ç•¥
    """
    with torch.no_grad():
        obs_tensor = self._obs_to_tensor(observation)
        
        if deterministic:
            # æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ (Î¼(s))
            action = self.actor_critic.get_action(obs_tensor, deterministic=True)
            return action.cpu().numpy(), None, None
        else:
            # è®­ç»ƒæ—¶ä½¿ç”¨éšæœºç­–ç•¥ (Ï€(a|s))
            action, log_prob = self.actor_critic.get_action(obs_tensor, deterministic=False)
            value = self.actor_critic.get_value(obs_tensor)
            return action.cpu().numpy(), log_prob.cpu(), value.cpu()
```

**åŠ¨ä½œé‡‡æ ·æµç¨‹**:
1. **ç¡®å®šæ€§**: `a = Î¼(s)` - ç›´æ¥è¾“å‡ºå‡å€¼
2. **éšæœºæ€§**: `a ~ Ï€(Â·|s) = N(Î¼(s), ÏƒÂ²(s))` - ä»æ­£æ€åˆ†å¸ƒé‡‡æ ·

## ğŸ”„ **4. PPOæ ¸å¿ƒæ›´æ–°ç®—æ³•**

### **PPO-Clipç›®æ ‡å‡½æ•°**
```python
def _train_networks(self) -> Dict:
    for epoch in range(self.n_epochs):
        # è·å–å½“å‰ç½‘ç»œè¾“å‡º
        values, log_probs, entropy = self.actor_critic.evaluate(obs_batch, actions_batch)
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        ratio = torch.exp(log_probs - old_log_probs_batch)
        
        # PPO-Clipç›®æ ‡å‡½æ•°
        surr1 = ratio * advantages_batch                    # æœªè£å‰ªé¡¹
        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon,   # è£å‰ªé¡¹
                           1 + self.clip_epsilon) * advantages_batch
        policy_loss = -torch.min(surr1, surr2).mean()       # å–æœ€å°å€¼(ä¿å®ˆä¼°è®¡)
```

**PPO-Clipæ•°å­¦å…¬å¼**:
```
L^{CLIP}(Î¸) = E[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)]

å…¶ä¸­:
- r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)  (é‡è¦æ€§é‡‡æ ·æ¯”ç‡)
- A_t: ä¼˜åŠ¿å‡½æ•°
- Îµ: è£å‰ªå‚æ•° (é€šå¸¸0.1-0.3)
```

### **ä»·å€¼å‡½æ•°æŸå¤±**
```python
# ä»·å€¼å‡½æ•°è£å‰ªæŸå¤± (PPOè®ºæ–‡ä¸­çš„æŠ€å·§)
if self.value_loss_coef > 0:
    value_pred_clipped = old_values_batch + torch.clamp(
        values - old_values_batch, -self.clip_epsilon, self.clip_epsilon
    )
    value_loss1 = (values - returns_batch).pow(2)              # æœªè£å‰ªæŸå¤±
    value_loss2 = (value_pred_clipped - returns_batch).pow(2)  # è£å‰ªæŸå¤±
    value_loss = 0.5 * torch.max(value_loss1, value_loss2).mean()  # å–æœ€å¤§å€¼
else:
    value_loss = 0.5 * (values - returns_batch).pow(2).mean()     # æ ‡å‡†MSEæŸå¤±
```

### **ç†µæ­£åˆ™åŒ–**
```python
# ç†µæŸå¤± (é¼“åŠ±æ¢ç´¢)
entropy_loss = -entropy.mean()

# æ€»æŸå¤±å‡½æ•°
total_loss = (policy_loss + 
              self.value_loss_coef * value_loss + 
              self.entropy_coef * entropy_loss)
```

**å®Œæ•´çš„PPOæŸå¤±å‡½æ•°**:
```
L_total = L^{CLIP} + câ‚L^{VF} + câ‚‚H(Ï€_Î¸)

å…¶ä¸­:
- L^{CLIP}: PPOç­–ç•¥æŸå¤±
- L^{VF}: ä»·å€¼å‡½æ•°æŸå¤±  
- H(Ï€_Î¸): ç­–ç•¥ç†µ
- câ‚, câ‚‚: æƒé‡ç³»æ•°
```

## ğŸ›¡ï¸ **5. è®­ç»ƒç¨³å®šæ€§æœºåˆ¶**

### **æ—©åœæœºåˆ¶**
```python
# KLæ•£åº¦ç›‘æ§
with torch.no_grad():
    kl_div = 0.5 * (log_probs - old_log_probs_batch).pow(2).mean()
    
# æ—©åœæ¡ä»¶
if kl_div > self.target_kl:
    print(f"Early stopping at epoch {epoch} due to reaching max KL: {kl_div:.4f}")
    break
```

### **æ¢¯åº¦è£å‰ª**
```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.max_grad_norm)
```

### **è§£é‡Šæ–¹å·®è®¡ç®—**
```python
def _explained_variance(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
    """
    è¡¡é‡ä»·å€¼å‡½æ•°çš„æ‹Ÿåˆè´¨é‡
    EV = 1 - Var(y_true - y_pred) / Var(y_true)
    """
    var_y = np.var(y_true)
    return 1 - np.var(y_true - y_pred) / (var_y + 1e-8)
```

## ğŸ“Š **6. è®­ç»ƒç›‘æ§å’Œç»Ÿè®¡**

```python
# å®æ—¶ç»Ÿè®¡ä¿¡æ¯
self.training_stats = {
    'policy_loss': deque(maxlen=100),      # ç­–ç•¥æŸå¤±
    'value_loss': deque(maxlen=100),       # ä»·å€¼æŸå¤±
    'entropy': deque(maxlen=100),          # ç­–ç•¥ç†µ
    'kl_divergence': deque(maxlen=100),    # KLæ•£åº¦
    'explained_variance': deque(maxlen=100), # è§£é‡Šæ–¹å·®
    'clipfrac': deque(maxlen=100)          # è£å‰ªæ¯”ä¾‹
}

# è£å‰ªæ¯”ä¾‹ç»Ÿè®¡
clipfrac = ((ratio - 1).abs() > self.clip_epsilon).float().mean()
```

## ğŸ’¾ **7. æ¨¡å‹æŒä¹…åŒ–**

```python
def save(self, filepath: str):
    """å®Œæ•´çš„æ£€æŸ¥ç‚¹ä¿å­˜"""
    torch.save({
        'actor_critic_state_dict': self.actor_critic.state_dict(),
        'optimizer_state_dict': self.optimizer.state_dict(),
        'scheduler_state_dict': self.scheduler.state_dict(),
        'total_steps': self.total_steps,
        'episode_count': self.episode_count,
        'training_stats': dict(self.training_stats)
    }, filepath)

def load(self, filepath: str):
    """å®Œæ•´çš„æ£€æŸ¥ç‚¹æ¢å¤"""
    checkpoint = torch.load(filepath, map_location=self.device)
    self.actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])
    # ... æ¢å¤æ‰€æœ‰çŠ¶æ€
```

## ğŸ”§ **8. å…³é”®è®¾è®¡ç‰¹ç‚¹**

### **ä¼˜åŠ¿**
1. **æ ‡å‡†PPOå®ç°**: ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡å®ç°ï¼ŒåŒ…å«æ‰€æœ‰å…³é”®æŠ€å·§
2. **å¤šæ¨¡æ€æ”¯æŒ**: æ”¯æŒå›¾åƒ+çŠ¶æ€çš„æ··åˆè§‚å¯Ÿç©ºé—´
3. **ç¨³å®šæ€§ä¿è¯**: GAEã€æ¢¯åº¦è£å‰ªã€æ—©åœã€KLç›‘æ§
4. **å®Œæ•´ç›‘æ§**: ä¸°å¯Œçš„è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
5. **çµæ´»é…ç½®**: å¤§é‡å¯è°ƒè¶…å‚æ•°

### **æ ¸å¿ƒç®—æ³•æµç¨‹**
```python
# PPOè®­ç»ƒå¾ªç¯
for episode in episodes:
    # 1. æ”¶é›†ç»éªŒ
    for step in range(buffer_size):
        action, log_prob, value = agent.get_action(obs)
        next_obs, reward, done = env.step(action)
        agent.store_transition(obs, action, reward, value, log_prob, done)
    
    # 2. è®¡ç®—ä¼˜åŠ¿å‡½æ•°
    agent.buffer.compute_advantages_and_returns(last_value)
    
    # 3. å¤šè½®ç­–ç•¥æ›´æ–°
    for epoch in range(n_epochs):
        # æ‰¹æ¬¡è®­ç»ƒ
        for batch in agent.buffer.get_batches():
            # PPOæŸå¤±è®¡ç®—å’Œåå‘ä¼ æ’­
            loss = compute_ppo_loss(batch)
            loss.backward()
            optimizer.step()
```

è¿™ä¸ªå®ç°éå¸¸å®Œæ•´å’Œä¸“ä¸šï¼ŒåŒ…å«äº†PPOç®—æ³•çš„æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å’Œæœ€ä½³å®è·µï¼